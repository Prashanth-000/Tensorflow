
--------------------------------------------------------------------------------------------------------------
    # program_1

    from re import VERBOSE
    import numpy as np
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Input
    from tensorflow.keras.optimizers import Adam

    x=np.array([[0,0],[0,1],[1,0],[1,1]])
    y=np.array([0,1,1,0])

    model=Sequential()
    model.add(Input(shape=(2,)))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1,activation='sigmoid'))

    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(learning_rate=0.1),
        metrics=['accuracy']
    )

    model.fit(x,y,epochs=1000,verbose=0)

    loss,accuracy=model.evaluate(x,y)
    print(f"Accuracy:{accuracy*100} %")

    predictions=model.predict(x)
    predictions=(predictions>0.5).astype(int)

    for i, prediction in enumerate(predictions):
      print(f"input {x[i]} predicted {prediction[0]} : actual {y[i]}")

-----------------------------------------------------------------------------------------------------------------------------

    # program_2
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, regularizers
    import numpy as np

    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0

    x_train = np.clip(x_train + 0.05 * np.random.normal(size=x_train.shape), 0, 1)

    x_train = np.expand_dims(x_train, -1)
    x_test  = np.expand_dims(x_test, -1)

    y_train = keras.utils.to_categorical(y_train, 10)
    y_test  = keras.utils.to_categorical(y_test, 10)

    datagen = keras.preprocessing.image.ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        validation_split=0.2
    )

    train_gen = datagen.flow(x_train, y_train, batch_size=128, subset="training")
    val_gen   = datagen.flow(x_train, y_train, batch_size=128, subset="validation")

    model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28, 1)),
        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),
        layers.Dropout(0.2),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),
        layers.Dropout(0.3),
        layers.Dense(10, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=[early])

----------------------------------------------------------------------------------------------------------------------------------

    # program_3
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, optimizers, losses
    import numpy as np
    import matplotlib.pyplot as plt

    X = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)
    y = np.array([[0], [1], [1], [0]], dtype=np.float32)

    def create_model():
        model = keras.Sequential([
            layers.Dense(4, input_dim=2, activation='sigmoid'),
            layers.Dense(1, activation='sigmoid')
        ])
        return model

    def train_model(optimizer_name, X, y, epochs=5000, lr=0.1):
        model = create_model()

        if optimizer_name == 'SGD':
            optimizer = optimizers.SGD(learning_rate=lr)
        elif optimizer_name == 'Momentum':
            optimizer = optimizers.SGD(learning_rate=lr, momentum=0.9)
        elif optimizer_name == 'Adam':
            optimizer = optimizers.Adam(learning_rate=lr)

        model.compile(optimizer=optimizer, loss=losses.MeanSquaredError())
        history = model.fit(X, y, epochs=epochs, verbose=0)

        return history.history['loss']

    losses_sgd = train_model('SGD', X, y)
    losses_momentum = train_model('Momentum', X, y)
    losses_adam = train_model('Adam', X, y)

    plt.plot(losses_sgd, label='SGD')
    plt.plot(losses_momentum, label='Momentum')
    plt.plot(losses_adam, label='Adam')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Optimization Algorithms Comparison (XOR Problem)')
    plt.show()

    print(f"Final Loss (SGD): {losses_sgd[-1]}")
    print(f"Final Loss (Momentum): {losses_momentum[-1]}")
    print(f"Final Loss (Adam): {losses_adam[-1]}")
------------------------------------------------------------------------------------------------------------------

    # program_4
    import tensorflow as tf
    from tensorflow.keras import layers, models
    from tensorflow.keras.datasets import mnist
    from tensorflow.keras.utils import to_categorical
    import matplotlib.pyplot as plt

    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype("float32") / 255
    x_test  = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype("float32") / 255

    y_train = to_categorical(y_train)
    y_test  = to_categorical(y_test)

    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation="relu", input_shape=(28, 28, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(64, activation="relu"),
        layers.Dense(10, activation="softmax")
    ])

    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )

    model.summary()

    history = model.fit(
        x_train, y_train,
        epochs=5,
        batch_size=64,
        validation_split=0.1
    )
    test_loss, test_acc = model.evaluate(x_test, y_test)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    plt.plot(history.history["accuracy"], label="Train Accuracy")
    plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
    plt.title("Training and Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)
    plt.show()
---------------------------------------------------------------------------------------------------------------------

    # program_5

    import tensorflow as tf
    from tensorflow.keras.datasets import imdb
    from tensorflow.keras import layers
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, LSTM, Dropout ,Dense
    import matplotlib.pyplot as plt

    vocab_size=10000
    sequence_len=200

    (x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=vocab_size)
    x_train=pad_sequences(x_train,maxlen=sequence_len ,padding='post')
    x_test=pad_sequences(x_test,maxlen=sequence_len,padding='post')

    model=Sequential([
        layers.Embedding(input_dim=vocab_size,output_dim=64,input_length=sequence_len),
        layers.LSTM(units=64,return_sequences=False),
        layers.Dropout(0.5),
        layers.Dense(1,activation='sigmoid')

    ])
    model.compile(
        loss='binary_crossentropy',
        optimizer='Adam',
        metrics=['accuracy']
    )
    model.summary()

    history=model.fit(
        x_train,
        y_train,
        epochs=5,
        batch_size=64,
        validation_split=0.2
    )

    loss,accuracy=model.evaluate(x_test,y_test)
    print(f"Accuracy is {accuracy*100} %")

    def plot_graph(history, string):
      plt.plot(history.history[string])
      plt.plot(history.history['val_'+ string])
      plt.xlabel("epochs")
      plt.ylabel(string)
      plt.legend()
      plt.title(f"{string} over epochs")
      plt.show()

    plot_graph(history,"loss")
    plot_graph(history,"accuracy")

----------------------------------------------------------------------------------------------------------------------
    # program_6

    import tensorflow as tf
    from tensorflow.keras import layers
    from tensorflow.keras.optimizers import Adam
    import numpy as np

    seq_length = 10
    num_samples = 1000

    X = []
    y = []
    for _ in range(num_samples):
        start = np.random.randint(0, 100)
        seq = np.arange(start, start + seq_length)
        X.append(seq[:-1])
        y.append(seq[1:])

    X = np.array(X, dtype=np.float32)[..., np.newaxis]
    y = np.array(y, dtype=np.float32)[..., np.newaxis]

    def build_simple_rnn(input_size=1, hidden_size=32, output_size=1):
        model = tf.keras.Sequential([
            layers.SimpleRNN(hidden_size, return_sequences=True, input_shape=(seq_length - 1, input_size)),
            layers.Dense(output_size)
        ])
        model.compile(optimizer=Adam(0.01), loss='mse')
        return model

    def build_birnn(input_size=1, hidden_size=32, output_size=1):
        model = tf.keras.Sequential([
            layers.Bidirectional(
                layers.SimpleRNN(hidden_size, return_sequences=True),
                input_shape=(seq_length - 1, input_size)
            ),
            layers.Dense(output_size)
        ])
        model.compile(optimizer=Adam(0.01), loss='mse')
        return model

    def train_model(model, X, y, epochs=50):
        history = model.fit(X, y, epochs=epochs, verbose=0)
        return history.history['loss'][-1]

    rnn_model = build_simple_rnn()
    birnn_model = build_birnn()

    rnn_loss = train_model(rnn_model, X, y)
    birnn_loss = train_model(birnn_model, X, y)

    print(f"Final Loss (Simple RNN): {rnn_loss:.4f}")
    print(f"Final Loss (BiRNN):      {birnn_loss:.4f}")
-------------------------------------------------------------------------------------------

    # program_7

    import numpy as np
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, LSTM, Dense

    num_samples = 1000
    timesteps = 5
    input_dim = 1
    latent_dim = 32

    X = np.random.rand(num_samples, timesteps, input_dim)
    Y = np.flip(X, axis=1)

    encoder_inputs = Input(shape=(timesteps, input_dim))
    encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_inputs)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(timesteps, input_dim))
    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
    decoder_dense = Dense(input_dim)
    decoder_outputs = decoder_dense(decoder_outputs)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    model.compile(optimizer='adam', loss='mse')

    model.fit([X, Y], Y, epochs=10, batch_size=32)

    pred = model.predict([X[:1], Y[:1]])

    print("Input:\n", X[0].squeeze())
    print("Predicted reversed:\n", pred[0].squeeze())
    print("True reversed:\n", Y[0].squeeze())

--------------------------------------------------------------------------------------------------------
    # program_8

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

(data, _), _ = tf.keras.datasets.mnist.load_data()
data = (data.reshape(-1, 784) / 255.0 > 0.5).astype("float32")[:10000]

# 2. Initialize RBM Parameters
W  = tf.Variable(tf.random.normal([784, 128], stddev=0.01))
hb = tf.Variable(tf.zeros([128]))
vb = tf.Variable(tf.zeros([784]))
 
def sample(x, W, b):
    prob = tf.sigmoid(tf.matmul(x, W) + b)
    return prob, tf.nn.relu(tf.sign(prob - tf.random.uniform(tf.shape(prob))))

losses = []
for epoch in range(6):
    epoch_loss = 0
    for i in range(0, len(data), 64):
        v0 = data[i:i+64]

        p_h0, h0 = sample(v0, W, hb)
        p_v1, v1 = sample(h0, tf.transpose(W), vb)
        p_h1, h1 = sample(v1, W, hb)

        W.assign_add(0.05 * (tf.matmul(tf.transpose(v0), p_h0) -
                             tf.matmul(tf.transpose(v1), p_h1)) / len(v0))
        vb.assign_add(0.05 * tf.reduce_mean(v0 - v1, axis=0))
        hb.assign_add(0.05 * tf.reduce_mean(p_h0 - p_h1, axis=0))

        epoch_loss += tf.reduce_mean(tf.square(v0 - v1))

    losses.append(epoch_loss / (len(data) / 64))
    print(f"Epoch {epoch+1}, Loss: {losses[-1]:.4f}")

plt.plot(losses)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("RBM Training Loss")
plt.show()

--------------------------------------------------------------------------------------------------

    # program_9
    import tensorflow as tf
    from tensorflow.keras import layers, models
    import numpy as np
    import matplotlib.pyplot as plt

    (x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    x_train, x_test = np.expand_dims(x_train, -1), np.expand_dims(x_test, -1)

    x_train_noisy = np.clip(x_train + 0.3 * np.random.normal(0, 1, x_train.shape), 0, 1)
    x_test_noisy = np.clip(x_test + 0.3 * np.random.normal(0, 1, x_test.shape), 0, 1)

    plt.figure(figsize=(10, 4))
    for i in range(10):
        plt.subplot(2, 10, i+1)
        plt.imshow(x_test[i].squeeze(), cmap='gray'); plt.axis('off')
        plt.subplot(2, 10, i+11)
        plt.imshow(x_test_noisy[i].squeeze(), cmap='gray'); plt.axis('off')
    plt.show()

    autoencoder = models.Sequential([
        layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(28,28,1)),
        layers.MaxPooling2D((2,2), padding='same'),
        layers.Conv2D(64, (3,3), activation='relu', padding='same'),
        layers.MaxPooling2D((2,2), padding='same'),
        layers.Conv2D(64, (3,3), activation='relu', padding='same'),
        layers.UpSampling2D((2,2)),
        layers.Conv2D(32, (3,3), activation='relu', padding='same'),
        layers.UpSampling2D((2,2)),
        layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')
    ])

    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
    autoencoder.fit(x_train_noisy, x_train, epochs=5, batch_size=256, validation_data=(x_test_noisy, x_test))

    decoded = autoencoder.predict(x_test_noisy)

    plt.figure(figsize=(10, 4))
    for i in range(10):
        plt.subplot(3, 10, i+1)
        plt.imshow(x_test[i].squeeze(), cmap='gray'); plt.axis('off')
        plt.subplot(3, 10, i+11)
        plt.imshow(x_test_noisy[i].squeeze(), cmap='gray'); plt.axis('off')
        plt.subplot(3, 10, i+21)
        plt.imshow(decoded[i].squeeze(), cmap='gray'); plt.axis('off')
    plt.show()
------------------------------------------------------------------------------------------------------

    # program_10

    import numpy as np
    import tensorflow as tf
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU
    import matplotlib.pyplot as plt
    
    (X, _), _ = tf.keras.datasets.mnist.load_data()
    X = (X.astype("float32") - 127.5) / 127.5
    X = np.expand_dims(X, -1)
    
    gen = Sequential([
        Dense(7*7*128, input_dim=100),
        LeakyReLU(0.2),
        Reshape((7, 7, 128)),
        Conv2DTranspose(64, (5, 5), strides=2, padding="same"),
        LeakyReLU(0.2),
        Conv2DTranspose(1, (5, 5), strides=2, padding="same", activation="tanh")
    ])
    
    disc = Sequential([
        Conv2D(64, (5, 5), strides=2, padding="same", input_shape=[28, 28, 1]),
        LeakyReLU(0.2),
        Conv2D(128, (5, 5), strides=2, padding="same"),
        LeakyReLU(0.2),
        Flatten(),
        Dense(1, activation="sigmoid")
    ])
    
    disc.compile(loss="binary_crossentropy", optimizer="adam")
    disc.trainable = False
    
    gan = Sequential([gen, disc])
    gan.compile(loss="binary_crossentropy", optimizer="adam")
    
    batch = 128
    for step in range(1000):
        noise = np.random.normal(0, 1, (batch, 100))
        fake = gen.predict(noise, verbose=0)
    
        real = X[np.random.randint(0, X.shape[0], batch)]
    
        d_loss = disc.train_on_batch(
            np.concatenate([real, fake]),
            np.concatenate([np.ones((batch, 1)), np.zeros((batch, 1))])
        )
    
        g_loss = gan.train_on_batch(noise, np.ones((batch, 1)))
    
        if step % 100 == 0:
            print(f"Step {step} | D Loss: {d_loss:.3f} | G Loss: {g_loss:.3f}")
            img = fake[0].reshape(28, 28)
            plt.imshow(img, cmap="gray")
            plt.show()

---------------------------------------------------------------------------------------------------------------------------
